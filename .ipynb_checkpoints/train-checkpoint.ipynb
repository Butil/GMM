{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn import mixture as mix\n",
    "from sklearn.externals import joblib\n",
    "import fnmatch\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center(img, crop_size):\n",
    "    s = img.shape\n",
    "    v = s[0]//2 - crop_size//2\n",
    "    h = s[1]//2 - crop_size//2\n",
    "    cropped_img = img[v:v + crop_size, h: h + crop_size]\n",
    "    return cropped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_patches(patches, shuffle=True):\n",
    "    if shuffle:\n",
    "        np.random.shuffle(patches)\n",
    "    patches = np.float32(patches)\n",
    "    # need to do this patch by patch, otherwise memory consuming\n",
    "    for row in xrange(patches.shape[0]):\n",
    "        patch = patches[row,:]\n",
    "        norm_patch = patch - np.mean(patch)\n",
    "        norm_patch = norm_patch - np.min(norm_patch)\n",
    "        norm_patch /= np.max(norm_patch)\n",
    "        patches[row,:]=norm_patch\n",
    "    patches[np.isnan(patches)] = 0\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patches(processed_set, patches_per_image, b, seed=None):\n",
    "    imagenames = os.listdir(processed_set)\n",
    "    total_num_of_patches = patches_per_image * len(imagenames)\n",
    "    patches = np.empty((total_num_of_patches, b * b), np.uint8)\n",
    "    np.random.seed(seed)\n",
    "    j = 0\n",
    "    for name in imagenames:\n",
    "        img = cv2.imread(processed_set + name,  0)\n",
    "        image_size = img.shape[0]\n",
    "        for i in xrange(patches_per_image):\n",
    "            m = np.random.randint(0, image_size - b + 1)\n",
    "            n = np.random.randint(0, image_size - b + 1)      \n",
    "            patches[j] = np.reshape(img[m:m + b, n:n + b], (1, b*b))\n",
    "            j += 1\n",
    "    patches = normalize_patches(patches)\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_GMM(n_components, processed_set, processing, Check_dir, patches_per_image, b):\n",
    "    if not os.path.exists(Check_dir):\n",
    "        os.makedirs(Check_dir)\n",
    "    checkpoints = fnmatch.filter(os.listdir(Check_dir), '*GMM_' + processing + '_*.pkl')\n",
    "    \n",
    "    if len(checkpoints) != 0:\n",
    "        if os.path.isfile(Check_dir + 'GMM_' + processing + '_final.pkl'):\n",
    "            return None, None, None\n",
    "        checkpoints.sort(key=len)\n",
    "        last_checkpoint = checkpoints[-1]\n",
    "        print 'Loading from ' + Check_dir + last_checkpoint + '...'\n",
    "        GMM = joblib.load(Check_dir + last_checkpoint)\n",
    "        re_ckpt_number = re.search(processing + '_(.*).pkl', last_checkpoint)\n",
    "        initial_i = int(re_ckpt_number.group(1))\n",
    "        patches = joblib.load(Check_dir + 'Patches_' + processing + '.pkl')\n",
    "    else:\n",
    "        initial_i = 0\n",
    "        GMM = mix.GaussianMixture(n_components=n_components, warm_start=True)\n",
    "        \n",
    "    patches_path = Check_dir + 'Patches_' + processing + '.pkl'\n",
    "    if os.path.isfile(patches_path):\n",
    "        patches = joblib.load(patches_path)\n",
    "    else:\n",
    "        print 'Generating patches...'\n",
    "        patches = generate_patches(processed_set, patches_per_image, b)\n",
    "        joblib.dump(patches, patches_path)\n",
    "        \n",
    "    return GMM, initial_i, patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(processed_set, image_size, n_components = 200, \\\n",
    "          b = 8, patches_per_image = 500, batch_size = 10000, \\\n",
    "          save_interval = 20):\n",
    "    Check_dir = '../Models/' + str(image_size) + '/'\n",
    "    processing = processed_set[12 + len(str(image_size)):-5]\n",
    "    GMM, initial_i, patches = initialize_GMM(n_components, processed_set, processing, \\\n",
    "                                             Check_dir, patches_per_image, b)  \n",
    "    if GMM is None:\n",
    "        print processing + ' with image size: ' + str(image_size) + ' already trained.'\n",
    "        return 0\n",
    "    total_num_of_patches = patches.shape[0]\n",
    "    initial_i /= batch_size\n",
    "    print 'Training ' + processing + ' with image size: ' + str(image_size) +'...'\n",
    "    for i in range(initial_i, total_num_of_patches / batch_size):\n",
    "        GMM.fit(patches[i * batch_size: (i+1) * batch_size])\n",
    "        if i % save_interval == 0:\n",
    "            print 'Saving GMM_' + processing + '_' + str((i+1) * batch_size) + '.pkl...'\n",
    "            joblib.dump(GMM, Check_dir + 'GMM_' + processing + '_' + str((i+1) * batch_size) + '.pkl')\n",
    "    GMM.fit(patches[-(total_num_of_patches % batch_size):])\n",
    "    print 'Saving GMM_' + processing + '_final.pkl...'\n",
    "    joblib.dump(GMM, Check_dir + 'GMM_' + processing + '_final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORI with image size: 16 already trained.\n",
      "GF with image size: 16 already trained.\n",
      "JPG with image size: 16 already trained.\n",
      "MF with image size: 16 already trained.\n",
      "RS with image size: 16 already trained.\n",
      "USM with image size: 16 already trained.\n",
      "WGN with image size: 16 already trained.\n",
      "Training for image size: 16 completed.\n",
      "ORI with image size: 32 already trained.\n",
      "GF with image size: 32 already trained.\n",
      "JPG with image size: 32 already trained.\n",
      "MF with image size: 32 already trained.\n",
      "RS with image size: 32 already trained.\n",
      "USM with image size: 32 already trained.\n",
      "WGN with image size: 32 already trained.\n",
      "Training for image size: 32 completed.\n",
      "ORI with image size: 512 already trained.\n",
      "GF with image size: 512 already trained.\n",
      "JPG with image size: 512 already trained.\n",
      "MF with image size: 512 already trained.\n",
      "Loading from ../Models/512/GMM_RS_10000.pkl...\n",
      "Training RS with image size: 512...\n",
      "Saving GMM_RS_210000.pkl...\n",
      "Saving GMM_RS_410000.pkl...\n",
      "Saving GMM_RS_610000.pkl...\n",
      "Saving GMM_RS_810000.pkl...\n",
      "Saving GMM_RS_1010000.pkl...\n",
      "Saving GMM_RS_1210000.pkl...\n",
      "Saving GMM_RS_final.pkl...\n",
      "Generating patches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training USM with image size: 512...\n",
      "Saving GMM_USM_10000.pkl...\n",
      "Saving GMM_USM_210000.pkl...\n",
      "Saving GMM_USM_410000.pkl...\n",
      "Saving GMM_USM_610000.pkl...\n",
      "Saving GMM_USM_810000.pkl...\n",
      "Saving GMM_USM_1010000.pkl...\n",
      "Saving GMM_USM_1210000.pkl...\n",
      "Saving GMM_USM_final.pkl...\n",
      "Generating patches...\n",
      "Training WGN with image size: 512...\n",
      "Saving GMM_WGN_10000.pkl...\n",
      "Saving GMM_WGN_210000.pkl...\n",
      "Saving GMM_WGN_410000.pkl...\n",
      "Saving GMM_WGN_610000.pkl...\n",
      "Saving GMM_WGN_810000.pkl...\n",
      "Saving GMM_WGN_1010000.pkl...\n",
      "Saving GMM_WGN_1210000.pkl...\n",
      "Saving GMM_WGN_final.pkl...\n",
      "Training for image size: 512 completed.\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "Processings = ['ORI', 'GF', 'JPG', 'MF', 'RS', 'USM', 'WGN']\n",
    "im_sizes = [16, 32, 512]\n",
    "for im_size in im_sizes:\n",
    "    Processings_TRN = ['../DataSet/' +str(im_size) + '/' + proc + '/TRN/' for proc in Processings]\n",
    "    for processed_set in Processings_TRN:\n",
    "        train(processed_set, image_size=im_size)\n",
    "    print 'Training for image size: ' + str(im_size) + ' completed.'\n",
    "print 'Training completed.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
